{"cells":[{"cell_type":"code","execution_count":91,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"]}],"source":["%load_ext autoreload\n","%autoreload 2\n","\n","from src.datasets import CustomDirAudioDataset\n","from torch.utils.data import DataLoader\n","from src.collate_fn.collate import collate_fn\n","from pathlib import Path\n","from torch import nn\n","import torch"]},{"cell_type":"code","execution_count":92,"metadata":{},"outputs":[],"source":["cfg = {\"preprocessing\": {\"sr\": 16000}}"]},{"cell_type":"code","execution_count":93,"metadata":{},"outputs":[],"source":["ds = CustomDirAudioDataset(\"/Users/arturgimranov/CS/fourth_year/dla_course/ss/train_mixtures\", cfg, None)"]},{"cell_type":"code","execution_count":95,"metadata":{},"outputs":[{"data":{"text/plain":["{'ref_audio': tensor([[ 8.5449e-04,  1.7395e-03,  2.3193e-03,  ..., -3.6621e-04,\n","          -2.7466e-04, -3.0518e-05]]),\n"," 'ref_duration': 9.41,\n"," 'ref_length': 150560,\n"," 'ref_path': '/Users/arturgimranov/CS/fourth_year/dla_course/ss/train_mixtures/refs/4719_1638_004020_0-ref.wav',\n"," 'mix_audio': tensor([[ 0.0013,  0.0007,  0.0011,  ..., -0.0010,  0.0060,  0.0132]]),\n"," 'mix_duration': 3.0,\n"," 'mix_length': 48000,\n"," 'mix_path': '/Users/arturgimranov/CS/fourth_year/dla_course/ss/train_mixtures/mix/4719_1638_004020_0-mixed.wav',\n"," 'target_audio': tensor([[0.0017, 0.0009, 0.0015,  ..., 0.0126, 0.0128, 0.0141]]),\n"," 'target_duration': 3.0,\n"," 'target_length': 48000,\n"," 'target_path': '/Users/arturgimranov/CS/fourth_year/dla_course/ss/train_mixtures/targets/4719_1638_004020_0-target.wav',\n"," 'ref_speaker_id': 4719,\n"," 'ref_target': 0}"]},"execution_count":95,"metadata":{},"output_type":"execute_result"}],"source":["ds[0]"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["dl = DataLoader(\n","        ds,\n","        batch_size=4,\n","        collate_fn=collate_fn,\n","        shuffle=True,\n","        num_workers=2,\n","        drop_last=True,\n","    )"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["batch = next(iter(dl))"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([4, 48000])"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["batch[\"mix_audio\"].shape"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["mix_audio = batch[\"mix_audio\"]"]},{"cell_type":"markdown","metadata":{},"source":["# SpeechEncoder"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["L1 = 40\n","L2 = 160\n","L3 = 320\n","N = 256\n","speaker_dim = 256\n","n_classes = 10"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["class SpeechEncoder(nn.Module):\n","    def __init__(self, L1, L2, L3, N):\n","        super().__init__()\n","        self.L1 = L1\n","        self.L2 = L2\n","        self.L3 = L3\n","        self.N = N\n","\n","        self.short_encoder = nn.Conv1d(1, N, L1, L1 // 2)\n","        self.middle_encoder = nn.Sequential(\n","            nn.ConstantPad1d((0, (L2 - L1)), 0), nn.Conv1d(1, N, L2, L1 // 2)\n","        )\n","        self.long_encoder = nn.Sequential(\n","            nn.ConstantPad1d((0, (L3 - L1)), 0), nn.Conv1d(1, N, L3, L1 // 2)\n","        )\n","\n","    def forward(self, x, return_tuple=False):\n","        x1 = self.short_encoder(x)\n","        x2 = self.middle_encoder(x)\n","        x3 = self.long_encoder(x)\n","\n","        assert x1.shape == x2.shape == x3.shape\n","        if return_tuple:\n","            return torch.cat([x1, x2, x3], dim=1), (x1, x2, x3)\n","        return torch.cat([x1, x2, x3], dim=1)\n","\n","    def _length_after(self, length_before):\n","        return (\n","            torch.div(length_before - self.L1, self.L1 // 2, rounding_mode=\"floor\")\n","            + 1\n","        )\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["class ChannelLayerNorm(nn.LayerNorm):\n","    def __init__(self, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","\n","    def forward(self, x):\n","        x = x.transpose(-1, -2)\n","        x = super().forward(x)\n","        x = x.transpose(-1, -2)\n","        return x"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["class ResNetBlock(nn.Module):\n","    def __init__(self, dim):\n","        super().__init__()\n","        self.body = nn.Sequential(\n","            nn.Conv1d(dim, dim, 1),\n","            nn.BatchNorm1d(dim),\n","            nn.PReLU(),\n","            nn.Conv1d(dim, dim, 1),\n","            nn.BatchNorm1d(dim),\n","        )\n","\n","        self.head = nn.Sequential(\n","            nn.PReLU(),\n","            nn.MaxPool1d(3)\n","        )\n","\n","\n","    def forward(self, x):\n","        return self.head(x + self.body(x))\n"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["class SpeakerEncoder(nn.Module):\n","    def __init__(\n","        self, N, speaker_dim, n_classes, speech_encoder, num_resnet_blocks=3\n","    ):\n","        super().__init__()\n","        self.speech_encoder = speech_encoder\n","        self.channel_layer_norm = ChannelLayerNorm(3 * N)\n","        self.conv1 = nn.Conv1d(3 * N, speaker_dim, 1)\n","\n","        self.num_resnet_blocks = num_resnet_blocks\n","        self.resnet_blocks = nn.Sequential(\n","            *[ResNetBlock(speaker_dim) for _ in range(num_resnet_blocks)]\n","        )\n","        self.conv2 = nn.Conv1d(speaker_dim, speaker_dim, 1)\n","        self.linear = nn.Linear(speaker_dim, n_classes)\n","\n","    def _length_after_resnet(self, length_before):\n","        return torch.div(\n","            length_before, (3**self.num_resnet_blocks), rounding_mode=\"floor\"\n","        )\n","\n","    def forward(self, x, x_lengths):\n","        x = self.conv1(self.channel_layer_norm(self.speech_encoder(x)))\n","        x = self.conv2(self.resnet_blocks(x))\n","        x = x.sum(dim=-1) / self._length_after_resnet(\n","            self.speech_encoder._length_after(x_lengths)\n","        )[:, None]\n","        logits = self.linear(x)\n","        return x, logits\n"]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[],"source":["class TCNBase(nn.Module):\n","    def __init__(self, in_channels, speaker_channels, out_channels, dilation, kernel_size):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Conv1d(in_channels + speaker_channels, out_channels, 1),\n","            nn.PReLU(),\n","            nn.GroupNorm(1, out_channels),\n","            nn.Conv1d(\n","                out_channels,\n","                out_channels,\n","                kernel_size,\n","                dilation=dilation,\n","                padding=\"same\",\n","            ),\n","            nn.PReLU(),\n","            nn.GroupNorm(1, out_channels),\n","            nn.Conv1d(out_channels, in_channels, 1),\n","        )\n","\n","    def forward(self, x):\n","        return x + self.net(x)\n","\n","class TCNBlock(TCNBase):\n","    def __init__(self, in_channels, out_channels, dilation, kernel_size):\n","        super().__init__(in_channels, 0, out_channels, dilation, kernel_size)\n","\n","class FirstTCNBlock(TCNBase):\n","    def __init__(self, in_channels, speaker_dim, out_channels, dilation, kernel_size):\n","        super().__init__(in_channels, speaker_dim, out_channels, dilation, kernel_size)\n","\n","    def forward(self, x, speaker_embedding):\n","        time_length = x.shape[-1]\n","        speaker_embedding = torch.unsqueeze(speaker_embedding, -1)\n","        speaker_embedding = speaker_embedding.repeat(1, 1, time_length)\n","        return x + self.net(torch.cat([x, speaker_embedding], dim=1))\n","\n","\n","class StackedTCN(nn.Module):\n","    def __init__(\n","        self, in_channels, speaker_dim, out_channels, kernel_size, num_blocks\n","    ):  \n","        super().__init__()\n","        self.first_block = FirstTCNBlock(\n","                in_channels, speaker_dim, out_channels, 1, kernel_size\n","            )\n","\n","        self.rest_blocks = nn.Sequential(\n","            *[\n","                TCNBlock(out_channels, out_channels, 2 ** i, kernel_size)\n","                for i in range(1, num_blocks)\n","            ]\n","        )\n","\n","    def forward(self, x, speaker_embedding):\n","        x = self.first_block(x, speaker_embedding)\n","        print(f'{x.shape=}')\n","        return self.rest_blocks(x)\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["speech_encoder = SpeechEncoder(L1, L2, L3, N)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["mix_encode, (y1, y2, y3) = speech_encoder(mix_audio.unsqueeze(1), return_tuple=True)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([4, 256, 2399])"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["ln = ChannelLayerNorm(3 * N)\n","conv1 = nn.Conv1d(3 * N, speaker_dim, 1)\n","mix_encode = conv1(ln(mix_encode))\n","mix_encode.shape"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["ref_audio = batch[\"ref_audio\"]\n","# ref_audio = speech_encoder(ref_audio.unsqueeze(1))\n","# ref_audio.shape"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["speaker_encoder = SpeakerEncoder(N, speaker_dim, n_classes, speech_encoder)"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["ref_lengths = torch.tensor([duration * 16000 for duration in batch[\"ref_duration\"]])"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["ref_audio, logits = speaker_encoder(ref_audio.unsqueeze(1), ref_lengths)"]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[],"source":["tcn = StackedTCN(N, speaker_dim, 256, 3, 3)"]},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["speaker_embedding.shape=torch.Size([4, 256, 2399])\n","x.shape=torch.Size([4, 256, 2399])\n"]}],"source":["ref_audio = tcn(mix_encode, ref_audio)"]},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([4, 256, 2399])"]},"execution_count":80,"metadata":{},"output_type":"execute_result"}],"source":["ref_audio.shape"]},{"cell_type":"code","execution_count":82,"metadata":{},"outputs":[],"source":["short_decoder = nn.ConvTranspose1d(N, 1, L1, L1 // 2)"]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([4, 1, 48000])"]},"execution_count":83,"metadata":{},"output_type":"execute_result"}],"source":["short_decoder(ref_audio).shape"]},{"cell_type":"code","execution_count":106,"metadata":{},"outputs":[],"source":["a = torch.randn((3, 4, 100))\n","c = nn.Conv1d(4, 6, 5, 2)"]},{"cell_type":"code","execution_count":107,"metadata":{},"outputs":[],"source":["c1 = nn.Conv1d(6, 4, 5, 2)"]},{"cell_type":"code","execution_count":108,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([3, 4, 100])\n","torch.Size([3, 6, 48])\n","torch.Size([3, 4, 22])\n"]}],"source":["print(a.shape)\n","print(c(a).shape)\n","print(c1(c(a)).shape)"]},{"cell_type":"code","execution_count":109,"metadata":{},"outputs":[],"source":["class SpeakerExtractor(nn.Module):\n","    def __init__(\n","        self,\n","        N,\n","        speech_encoder,\n","        speaker_dim,\n","        tcn_block_dim,\n","        tcn_kernel_size,\n","        tcn_num_blocks,\n","        tnc_num_stacks, \n","    ):\n","        super().__init__()\n","        self.speech_encoder = speech_encoder\n","        self.channel_layer_norm = ChannelLayerNorm(3 * N)\n","        self.conv1 = nn.Conv1d(3 * N, speaker_dim, 1)\n","        self.tcn = nn.Sequential(\n","            *[\n","                StackedTCN(\n","                    speaker_dim,\n","                    speaker_dim,\n","                    tcn_block_dim,\n","                    tcn_kernel_size,\n","                    tcn_num_blocks,\n","                )\n","                for _ in range(tnc_num_stacks)\n","            ]\n","        )\n","        self.masks_head = nn.ModuleList(\n","            [\n","                nn.Sequential(\n","                    nn.Conv1d(speaker_dim, N, 1),\n","                    nn.ReLU(),\n","                )\n","            ] for i in range(3)\n","        )\n","\n","    def forward(self, mix, speaker_embedding):\n","        mix = self.conv1(self.channel_layer_norm(self.speech_encoder(mix)))\n","        mix_embedding = self.tcn(mix, speaker_embedding)\n","        masks = [mask_head(mix_embedding) for mask_head in self.masks_head]\n","        return [mask * mix for mask in masks]"]},{"cell_type":"code","execution_count":111,"metadata":{},"outputs":[],"source":["class SpeechDecoder(nn.Module):\n","    def __init__(self, L1, L2, L3, N):\n","        super().__init__()\n","        self.short_decoder = nn.ConvTranspose1d(N, 1, L1, L1 // 2)\n","        self.middle_decoder = nn.Sequential(\n","            nn.ConvTranspose1d(N, 1, L2, L2 // 2),\n","        )\n","        self.long_decoder = nn.Sequential(\n","            nn.ConvTranspose1d(N, 1, L3, L3 // 2),\n","        )\n","\n","    def forward(self, y1, y2, y3):\n","        y1 = self.short_decoder(y1)\n","        y2 = self.middle_decoder(y2)\n","        y3 = self.long_decoder(y3)\n","\n","        return y1, y2, y3\n","        "]},{"cell_type":"code","execution_count":112,"metadata":{},"outputs":[],"source":["class SpExPlus(nn.Module):\n","    def __init__(\n","        self,\n","        speaker_dim,\n","        tcn_block_dim,\n","        tcn_kernel_size,\n","        tcn_num_blocks,\n","        tnc_num_stacks,\n","        L1,\n","        L2,\n","        L3,\n","        N,\n","    ):\n","        super().__init__()\n","        self.speech_encoder = SpeechEncoder(L1, L2, L3, N)\n","        self.speaker_encoder = SpeakerEncoder(N, speaker_dim, n_classes, speech_encoder)\n","        self.speaker_extractor = SpeakerExtractor(\n","            N,\n","            speech_encoder,\n","            speaker_dim,\n","            tcn_block_dim,\n","            tcn_kernel_size,\n","            tcn_num_blocks,\n","            tnc_num_stacks,\n","        )\n","        self.speech_decoder = SpeechDecoder(L1, L2, L3, N)\n","\n","    def forward(self, mix, ref, ref_lengths):\n","        mix_length = mix.shape[-1]\n","        mix_encode, (y1, y2, y3) = self.speech_encoder(mix.unsqueeze(1), return_tuple=True)\n","        ref_encode = self.speech_encoder(ref.unsqueeze(1))\n","        ref_audio, logits = self.speaker_encoder(ref_encode, ref_lengths)\n","        masks = self.speaker_extractor(mix_encode, ref_audio)\n","\n","        for mask, y in zip(masks, [y1, y2, y3]):\n","            mask = torch.unsqueeze(mask, 1)\n","            mask = mask.repeat(1, N, 1)\n","            y = y * mask\n","\n","        short, middle, long = self.speech_decoder(y1, y2, y3)\n","\n","        return {\n","            \"short\": short[:, :, :mix_length],\n","            \"middle\": middle[:, :, :mix_length],\n","            \"long\": long[:, :, :mix_length],\n","        }\n"]},{"cell_type":"code","execution_count":301,"metadata":{},"outputs":[],"source":["speach_encoder = SpeechEncoder(L1, L2, L3, N)"]},{"cell_type":"code","execution_count":302,"metadata":{},"outputs":[],"source":["x = speach_encoder(mix_audio.unsqueeze(1))"]},{"cell_type":"code","execution_count":303,"metadata":{},"outputs":[],"source":["tcn_block = TCNBlock(3 * N, 3 * N, 2, 3)"]},{"cell_type":"code","execution_count":304,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([4, 768, 2399])"]},"execution_count":304,"metadata":{},"output_type":"execute_result"}],"source":["tcn_block(x).shape"]},{"cell_type":"code","execution_count":305,"metadata":{},"outputs":[],"source":["class MyModule(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x, y):\n","        return x + y"]},{"cell_type":"code","execution_count":276,"metadata":{},"outputs":[],"source":["s = nn.Sequential(MyModule(), nn.Linear(5, 10))"]},{"cell_type":"code","execution_count":281,"metadata":{},"outputs":[{"ename":"TypeError","evalue":"forward() takes 2 positional arguments but 3 were given","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m/Users/arturgimranov/CS/fourth_year/dla_course/ss/sandbox_model.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/arturgimranov/CS/fourth_year/dla_course/ss/sandbox_model.ipynb#Y116sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m s(torch\u001b[39m.\u001b[39;49mrandn((\u001b[39m3\u001b[39;49m, \u001b[39m4\u001b[39;49m, \u001b[39m5\u001b[39;49m)), torch\u001b[39m.\u001b[39;49mrandn((\u001b[39m3\u001b[39;49m, \u001b[39m4\u001b[39;49m, \u001b[39m5\u001b[39;49m)))\n","File \u001b[0;32m~/CS/fourth_year/dla_course/ss/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","\u001b[0;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given"]}],"source":["s(torch.randn((3, 4, 5)), torch.randn((3, 4, 5)))"]},{"cell_type":"code","execution_count":268,"metadata":{},"outputs":[],"source":["class Test:\n","    def __init__(self, a, *args, **kwargs):\n","        print(f'{a=}')\n","        print(f'{args=}')\n","        print(f'{kwargs=}')"]},{"cell_type":"code","execution_count":271,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["a=10\n","args=(20, 30)\n","kwargs={'b': 2}\n"]}],"source":["a = Test(10, 20, 30, b=2)"]},{"cell_type":"code","execution_count":231,"metadata":{},"outputs":[],"source":["x = torch.randn(3, 4, 10)\n","aux = torch.randn(3, 4)"]},{"cell_type":"code","execution_count":232,"metadata":{},"outputs":[],"source":["T = x.shape[-1]\n","aux = torch.unsqueeze(aux, -1)\n","aux = aux.repeat(1,1,T)"]},{"cell_type":"code","execution_count":239,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[-1.4661, -0.8354, -1.2423, -0.2026],\n","        [-0.1847, -0.1209,  1.6513, -0.1893],\n","        [-0.2205,  0.3992,  0.1901,  2.6755]])"]},"execution_count":239,"metadata":{},"output_type":"execute_result"}],"source":["aux[:, :, 0]"]},{"cell_type":"code","execution_count":240,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[-1.4661, -0.8354, -1.2423, -0.2026],\n","        [-0.1847, -0.1209,  1.6513, -0.1893],\n","        [-0.2205,  0.3992,  0.1901,  2.6755]])"]},"execution_count":240,"metadata":{},"output_type":"execute_result"}],"source":["aux[:, :, 1]"]},{"cell_type":"code","execution_count":233,"metadata":{},"outputs":[],"source":["y = torch.cat([x, aux], 1)"]},{"cell_type":"code","execution_count":235,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([3, 8, 10])"]},"execution_count":235,"metadata":{},"output_type":"execute_result"}],"source":["y.shape"]},{"cell_type":"code","execution_count":259,"metadata":{},"outputs":[],"source":["gln = nn.GroupNorm(1, 8)"]},{"cell_type":"code","execution_count":261,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([1.0063, 1.0063, 1.0063], grad_fn=<StdBackward0>)"]},"execution_count":261,"metadata":{},"output_type":"execute_result"}],"source":["gln(y).std((1, 2))"]},{"cell_type":"code","execution_count":245,"metadata":{},"outputs":[{"ename":"AttributeError","evalue":"'LayerNorm' object has no attribute 'mean'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32m/Users/arturgimranov/CS/fourth_year/dla_course/ss/sandbox_model.ipynb Cell 28\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/arturgimranov/CS/fourth_year/dla_course/ss/sandbox_model.ipynb#Y112sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m gln\u001b[39m.\u001b[39;49mmean()\n","File \u001b[0;32m~/CS/fourth_year/dla_course/ss/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1185\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1183\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1184\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1185\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1186\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n","\u001b[0;31mAttributeError\u001b[0m: 'LayerNorm' object has no attribute 'mean'"]}],"source":["gln.mean()"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["from src.model import SpExPlus"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["model = SpExPlus(\n","    speaker_dim=256,\n","    tcn_block_dim=256,\n","    tcn_kernel_size=3,\n","    tcn_num_blocks=3,\n","    tcn_num_stacks=3,\n","    L1=40,\n","    L2=160,\n","    L3=320,\n","    N=256,\n","    n_classes=10,\n",")"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["mix_audio = batch[\"mix_audio\"]\n","ref_audio = batch[\"ref_audio\"]\n","ref_lengths = torch.tensor([duration * 16000 for duration in batch[\"ref_duration\"]])"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["x.shape=torch.Size([4, 256, 393])\n","x.shape=torch.Size([4, 256])\n"]}],"source":["a = model(mix_audio, ref_audio, ref_lengths)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["short torch.Size([4, 1, 48000])\n","middle torch.Size([4, 1, 48000])\n","long torch.Size([4, 1, 48000])\n","logits torch.Size([4, 10])\n"]}],"source":["for key, value in a.items():\n","    print(key, value.shape)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([4, 212480])"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["ref_audio.shape"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[],"source":["def si_sdr(est, target, eps=1e-8):\n","    l2norm = lambda x, keepdim=False: torch.norm(x, dim=-1, keepdim=keepdim)\n","    alpha = (target * est).sum() / l2norm(target, True)**2\n","    print(f'{alpha.shape=}')\n","    return 20 * torch.log10(l2norm(alpha * target) / (l2norm(alpha * target - est) + eps) + eps)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[{"data":{"text/plain":["(torch.Size([4, 48000]), torch.Size([4, 48000]))"]},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"source":["a[\"short\"].shape, mix_audio.shape"]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["alpha.shape=torch.Size([4, 1])\n"]},{"data":{"text/plain":["tensor([-35.6926, -35.7528, -34.7097, -34.3358], grad_fn=<MulBackward0>)"]},"execution_count":71,"metadata":{},"output_type":"execute_result"}],"source":["si_sdr(a[\"short\"] - a[\"short\"].mean(dim=-1, keepdim=True), mix_audio - mix_audio.mean(dim=-1, keepdim=True))"]},{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[],"source":["def sisdr(x, s, eps=1e-8):\n","    \"\"\"\n","    Arguments:\n","    x: separated signal, N x S tensor\n","    s: reference signal, N x S tensor\n","    Return:\n","    sisdr: N tensor\n","    \"\"\"\n","\n","    def l2norm(mat, keepdim=False):\n","        return torch.norm(mat, dim=-1, keepdim=keepdim)\n","\n","    x_zm = x - torch.mean(x, dim=-1, keepdim=True)\n","    s_zm = s - torch.mean(s, dim=-1, keepdim=True)\n","    t = torch.sum(\n","        x_zm * s_zm, dim=-1,\n","        keepdim=True) * s_zm / (l2norm(s_zm, keepdim=True)**2 + eps)\n","    return 20 * torch.log10(eps + l2norm(t) / (l2norm(x_zm - t) + eps))"]},{"cell_type":"code","execution_count":82,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([-42.4574, -36.3025, -35.8277, -40.6908], grad_fn=<MulBackward0>)"]},"execution_count":82,"metadata":{},"output_type":"execute_result"}],"source":["sisdr(a[\"short\"], mix_audio)"]},{"cell_type":"code","execution_count":89,"metadata":{},"outputs":[],"source":["from functools import partial\n","\n","def si_sdr(estimated, target, eps=1e-8):\n","    l2norm = partial(torch.linalg.norm, dim=-1)\n","    alpha = (target * estimated).sum(dim=-1, keepdim=True) / l2norm(target, keepdim=True) ** 2\n","    return 20 * torch.log10(l2norm(alpha * target) / (l2norm(alpha * target - estimated) + eps) + eps)"]},{"cell_type":"code","execution_count":90,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([-42.4574, -36.3025, -35.8277, -40.6908], grad_fn=<MulBackward0>)"]},"execution_count":90,"metadata":{},"output_type":"execute_result"}],"source":["si_sdr(a[\"short\"] - a[\"short\"].mean(-1, keepdim=True), mix_audio - mix_audio.mean(-1, keepdim=True))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"ss","language":"python","name":"ss"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":2}
