import logging
import random
import string
from pathlib import Path
from typing import List

import numpy as np
import torch
import torchaudio
from hydra.utils import instantiate
from omegaconf import DictConfig
from torch import Tensor
from torch.utils.data import Dataset

from src.text_encoder import BaseTextEncoder

logger = logging.getLogger(__name__)


class BaseDataset(Dataset):
    """
    Realizes all dataset logic on a given index.
    Indexes should be generated by inheriting classes.
    """

    def __init__(self, index, cfg, wave_augs, limit=None):
        self.cfg = cfg
        self.wave_augs = wave_augs
        self._assert_index_is_valid(index)
        # it's a good idea to sort index by audio length
        # It would be easier to write length-based batch samplers later
        # index = self._sort_index(index)
        if limit is not None:
            random.seed(42)
            random.shuffle(index)
            self._index: List[dict] = index[:limit]
        else:
            self._index: List[dict] = index

    def __getitem__(self, ind):
        data_dict = self._index[ind]
        item_dict = {}
        for audio_type in ["ref", "mix", "target"]:
            audio_path = data_dict[f"{audio_type}_path"]
            audio_wave = self.process_wave(self.load_audio(audio_path))
            type_dict = {
                f"{audio_type}_audio": audio_wave,
                f"{audio_type}_duration": audio_wave.shape[1]
                / self.cfg["preprocessing"]["sr"],
                f"{audio_type}_length": audio_wave.shape[1],
                f"{audio_type}_path": audio_path,
            }
            item_dict.update(type_dict)
        item_dict["ref_speaker_id"] = data_dict["ref_speaker_id"]
        item_dict["ref_target"] = data_dict["ref_target"]
        return item_dict

    @staticmethod
    def _sort_index(index):
        return sorted(index, key=lambda x: x["ref_duration"])

    def __len__(self):
        return len(self._index)

    def load_audio(self, path):
        audio_tensor, sr = torchaudio.load(path)
        audio_tensor = audio_tensor[0:1, :]  # remove all channels but the first
        target_sr = self.cfg["preprocessing"]["sr"]
        if sr != target_sr:
            audio_tensor = torchaudio.functional.resample(audio_tensor, sr, target_sr)
        return audio_tensor

    def process_wave(self, audio_tensor_wave: Tensor):
        with torch.no_grad():
            if self.wave_augs is not None:
                audio_tensor_wave = self.wave_augs(audio_tensor_wave)
            return audio_tensor_wave

    @staticmethod
    def _assert_index_is_valid(index):
        for entry in index:
            required_fields = ["ref_path", "mix_path", "target_path"]
            for field in required_fields:
                assert field in entry, (
                    "Each dataset item should include field "
                    f"'{field}' - path to audio file."
                )
